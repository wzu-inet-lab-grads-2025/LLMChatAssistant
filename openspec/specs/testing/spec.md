# 测试规范

本规范定义了后端功能审阅的测试要求、测试范围和验收标准。

## 新增需求

### 需求：测试套件覆盖率
系统必须提供全面的测试套件，用于审阅和验证后端所有功能的实现正确性。

#### 场景：测试覆盖率达到90%
- **当** 运行完整的测试套件
- **那么** 代码覆盖率必须 ≥ 90%
- **并且** 关键模块覆盖率（agent.py）必须 ≥ 95%

#### 场景：测试包含多个层次
- **当** 设计测试架构
- **那么** 必须包含单元测试（20-30%）、集成测试（70-75%）、端到端测试（5-10%）

#### 场景：每个功能至少5个测试样例
- **当** 编写功能测试
- **那么** 每个功能必须至少包含5个不同的测试样例
- **并且** 样例必须覆盖正常场景和异常场景

### 需求：Agent意图识别测试
系统必须验证ReAct Agent的意图识别能力，确保能正确区分不同的用户输入场景。

#### 场景：区分自然语言命令和命令询问
- **当** 用户输入"查看配置文件内容"
- **那么** Agent应识别为需要执行命令（可能先搜索再执行）
- **当** 用户输入"介绍一下df指令"
- **那么** Agent应识别为需要直接聊天，不调用任何工具

#### 场景：区分直接命令和命令询问
- **当** 用户输入"ls -la"
- **那么** Agent应调用command_executor工具
- **当** 用户输入"ls命令有什么用"
- **那么** Agent应直接聊天，不调用工具

#### 场景：意图识别准确率统计
- **当** 运行完整的意图识别测试集（至少75个测试样例）
- **那么** 意图识别准确率必须 ≥ 90%
- **并且** 必须生成各场景的准确率统计报告

### 需求：多工具协作测试
系统必须验证Agent的多工具协作能力，确保能正确执行需要2-3轮工具调用的复杂任务。

#### 场景：查看配置文件内容
- **当** 用户输入"查看配置文件内容"
- **那么** Agent应：
  1. 第1轮：调用semantic_search搜索配置文件
  2. 第2轮：调用command_executor执行cat命令显示文件内容
  3. 生成最终回复

#### 场景：搜索日志中的错误信息
- **当** 用户输入"搜索日志中的错误信息"
- **那么** Agent应：
  1. 第1轮：调用semantic_search搜索日志文件
  2. 第2轮：调用command_executor执行grep命令搜索错误
  3. 基于搜索结果生成回复

#### 场景：多工具协作成功率统计
- **当** 运行完整的多工具协作测试集（至少25个测试样例）
- **那么** 多工具协作成功率必须 ≥ 85%

### 需求：工具链功能测试
系统必须验证所有工具（5个）的功能正确性和参数验证。

#### 场景：command_executor工具测试
- **当** 测试command_executor工具
- **那么** 必须测试所有白名单命令（ls, cat, grep, head, tail, ps, pwd, whoami, df, free）
- **并且** 每个命令至少5个测试样例
- **并且** 必须测试白名单验证、黑名单字符验证、路径验证

#### 场景：sys_monitor工具测试
- **当** 测试sys_monitor工具
- **那么** 必须测试所有指标（cpu, memory, disk, all）
- **并且** 每个指标至少5个测试样例

#### 场景：semantic_search工具测试
- **当** 测试semantic_search工具
- **那么** 必须测试混合检索策略（精确匹配→模糊匹配→语义检索）
- **并且** 每种匹配方式至少5个测试样例
- **并且** 必须测试scope参数（all, system, uploads）

#### 场景：file_upload工具测试
- **当** 测试file_upload工具
- **那么** 必须测试所有action（list）
- **并且** 必须测试所有reference类型（all, this, these, previous）
- **并且** 必须测试文件类型过滤

#### 场景：file_download工具测试
- **当** 测试file_download工具
- **那么** 必须测试文件路径验证
- **并且** 必须测试RDT下载提议生成
- **并且** 必须测试client_type适配（cli/web）

### 需求：后端与前端通信测试
系统必须验证后端能正确地向前端传递消息，符合NPLT协议规范。

#### 场景：NPLT消息编码解码
- **当** 测试NPLT协议消息
- **那么** 必须验证消息的Type, Seq, Len, Data字段正确性
- **并且** 必须测试所有消息类型（CHAT_TEXT, AGENT_THOUGHT, DOWNLOAD_OFFER等）

#### 场景：流式输出分块传输
- **当** Agent生成流式输出
- **那么** 后端必须正确发送STREAM_START标记
- **并且** 必须将内容分块发送（每块10-50字符）
- **并且** 必须发送STREAM_END标记（空CHAT_TEXT消息）

#### 场景：状态通知消息
- **当** Agent执行工具调用
- **那么** 后端必须发送状态通知：
  - thinking：正在分析意图
  - tool_call：正在调用工具
  - generating：正在生成回复

### 需求：历史聊天记录持久化测试
系统必须验证对话历史能正确保存到磁盘并在会话恢复后加载。

#### 场景：对话历史保存
- **当** 完成一轮对话
- **那么** 对话历史必须保存到storage/history/{session_id}.json
- **并且** 文件格式必须符合规范（role, content, timestamp, tool_calls, metadata）

#### 场景：工具调用记录保存
- **当** Agent调用工具
- **那么** 工具调用信息必须保存到历史记录的tool_calls字段
- **并且** 必须包含tool_name, arguments, result, status, duration, timestamp

#### 场景：会话恢复后历史加载
- **当** 重新连接服务器并切换会话
- **那么** 必须从磁盘加载该会话的历史记录
- **并且** 历史记录必须完整且格式正确

### 需求：文件上传、存储、索引测试
系统必须验证文件上传、存储、索引的完整流程。

#### 场景：文件上传到正确路径
- **当** 客户端上传文件
- **那么** 文件必须保存到storage/uploads/{file_id}/目录
- **并且** 文件名必须保持原样

#### 场景：文件元数据保存
- **当** 文件上传完成
- **那么** 文件元数据必须添加到session.uploaded_files
- **并且** 必须同步到conversation_history.uploaded_files
- **并且** 元数据必须包含：file_id, filename, file_path, size, uploaded_at, indexed

#### 场景：白名单文件自动索引
- **当** 上传的文件在白名单中
- **那么** 系统必须自动触发向量索引
- **并且** 索引完成后indexed字段必须为true

### 需求：上下文管理测试
系统必须验证对话历史和工具调用结果能正确传递给LLM。

#### 场景：对话历史传递给LLM
- **当** Agent调用LLM生成回复
- **那么** 必须传递最近N轮对话历史（默认N=5）
- **并且** 格式必须符合LLM API要求（role, content）

#### 场景：工具调用结果加入上下文
- **当** Agent执行工具调用
- **那么** 工具调用结果必须加入到下一轮LLM请求的上下文中
- **并且** 最终回复必须基于工具调用结果生成

### 需求：上传文件后的上下文管理测试
系统必须验证上传的文件能通过session对象传递给工具，并支持代词引用。

#### 场景：uploaded_files通过session传递
- **当** 工具需要访问用户上传的文件
- **那么** session对象必须包含uploaded_files字段
- **并且** 工具可以通过session.uploaded_files访问文件列表

#### 场景：文件代词引用解析
- **当** 用户输入"这个文件"、"这两个文件"、"之前上传的"
- **那么** file_upload工具必须正确解析代词引用
- **并且** 返回正确的文件路径

### 需求：错误处理测试
系统必须验证各种错误情况的处理能力。

#### 场景：API调用失败降级
- **当** 智谱API调用失败
- **那么** Agent必须降级到本地模式
- **并且** 必须返回降级提示信息

#### 场景：工具执行超时
- **当** 工具执行时间超过超时限制（默认5秒）
- **那么** 系统必须返回超时错误
- **并且** Agent必须尝试其他方法或返回错误提示

#### 场景：参数验证失败
- **当** 工具参数验证失败
- **那么** 工具必须返回验证错误信息
- **并且** Agent不能调用该工具

### 需求：测试报告生成
系统必须生成详细的测试报告，包含所有测试样例的完整信息。

#### 场景：测试报告包含完整信息
- **当** 生成测试报告
- **那么** 每个测试样例必须包含：
  - 测试输入
  - API请求原始数据（messages, temperature等）
  - API响应原始数据（choices, usage等）
  - 工具调用决策过程（第1轮、第2轮...）
  - 每个工具的调用参数和返回结果
  - 传递给前端的消息序列（完整NPLT消息）
  - 保存到磁盘的历史记录（完整JSON）
  - 测试结果（通过/失败/部分通过）
  - 问题描述（如果失败）

#### 场景：测试报告包含统计分析
- **当** 生成测试报告
- **那么** 报告必须包含：
  - 总体统计（总样例数、通过率、失败率）
  - 意图识别准确率（按场景分类）
  - 多工具协作成功率
  - 测试覆盖率（按模块）
  - 性能指标（响应时间、API使用量）

### 需求：Agent的system_prompt优化
系统必须通过多轮测试优化Agent的system_prompt，提高意图识别和多工具协作的成功率。

#### 场景：多轮测试迭代优化
- **当** 进行prompt优化
- **那么** 必须执行至少3轮测试：
  1. 第1轮：测试当前prompt
  2. 第2轮：根据第1轮结果优化prompt并测试
  3. 第3轮：如果需要，继续微调并测试

#### 场景：选择最优prompt并更新
- **当** 完成多轮测试
- **那么** 必须选择综合效果最好的prompt版本
- **并且** 必须更新到agent.py文件
- **并且** 必须在测试报告中说明选择理由

#### 场景：Prompt优化记录
- **当** 生成测试报告
- **那么** 报告必须包含：
  - 每个prompt版本的测试结果对比
  - 意图识别准确率变化
  - 多工具协作成功率变化
  - 具体改进点分析
